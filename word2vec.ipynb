{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Vector Representations: word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computational linguistics\n",
    "\n",
    "The vast majority of rule-based and statistical NLP work regards words as atomic symbols. In vector space terms, this is a vector with one 1 and a lot of zeroes. We call this a “one-hot” representation. It is a localist representation.\n",
    "\n",
    "## From symbolic to distributed representations\n",
    "\n",
    "Its problem, e.g., for web search\n",
    "\n",
    "If user searches for [Dell notebook battery size], we would like to match documents with “Dell laptop battery capacity”\n",
    "\n",
    "If user searches for [Seattle motel], we would like to match documents containing “Seattle hotel”\n",
    "\n",
    "There is no natural notion of similarity in a set of one-hot vectors.\n",
    "\n",
    "两个向量的点乘可以反映两个向量的相似度，如果点乘为零，说明两个向量垂直，相互独立，毫无关系。当然，用欧拉距离或者其他距离也可以反映两个向量的相似度。\n",
    "\n",
    "## Distributional similarity based representations\n",
    "\n",
    "You can get a lot of value by representing a word by means of its neighbors\n",
    "\n",
    "You shall know a word by the company it keeps\n",
    "\n",
    "One of the most successful ideas of modern statistical NLP\n",
    "\n",
    "We will build a dense vector for each word type, chosen so that it is good at predicting other words appearing in its context\n",
    "\n",
    "The idea of distributional similarity is a theory about semantics of word\n",
    "\n",
    "distributed (distributional) representation vs. denotational representation (one-hot vector)\n",
    "\n",
    "Distributed representation (condense vector) is built by distributional similarity\n",
    "\n",
    "## Basic idea of learning neural network word embeddings\n",
    "\n",
    "We define a model that aims to predict between a center word wt and context words in terms of word vectors\n",
    "\n",
    "## Main idea of word2vec\n",
    "\n",
    "## Skip-gram prediction\n",
    "\n",
    "## Details of word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Markdown输入数学公式](http://oiltang.com/2014/05/04/markdown-and-mathjax/)\n",
    "\n",
    "* 123\n",
    "\n",
    "u<sup>T</sup>v = u * v = \\Omega u<sub>i</sub>v<sub>i</sub>\n",
    "\n",
    "u<sup>T</sup>v = u * v = sum(u<sub>i</sub>v<sub>i</sub>)\n",
    "\n",
    "p<sub>i</sub> = exp(u<sub>i</sub>)/sum(u<sub>j</sub>)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
